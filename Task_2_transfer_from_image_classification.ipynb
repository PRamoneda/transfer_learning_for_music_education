{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TASK2: transfer_from_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "34493a939e5d4a71b6b10d0f96130cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_4f5195375efd46509ce2de6ae8f77bd3",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_3a4970e6ffc64c41b0e6f94a3d86b24e",
       "IPY_MODEL_3f6a7227d6a040d98a38e4cd0c5760ee"
      ]
     }
    },
    "4f5195375efd46509ce2de6ae8f77bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3a4970e6ffc64c41b0e6f94a3d86b24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_d882e319ec744f989554171cc867a348",
      "_dom_classes": [],
      "description": "100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 46827520,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 46827520,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_c63a1295d3ef4d0bbc626a4aad3621f5"
     }
    },
    "3f6a7227d6a040d98a38e4cd0c5760ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_3769ee44482b4ead80efeff58eaa8eb9",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 58.2MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_48b1546a464a4642a257fa1e92bee31f"
     }
    },
    "d882e319ec744f989554171cc867a348": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "c63a1295d3ef4d0bbc626a4aad3621f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3769ee44482b4ead80efeff58eaa8eb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "48b1546a464a4642a257fa1e92bee31f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "073LNgg-Prk7"
   },
   "source": [
    "## TASK 2: transfer from image classification\n",
    "\n",
    "Image recognition is a machine learning area with several publications and data. The image recognition models are trained on much larger datasets than the music education systems~\\cite{he2016deep}. The strategy is to convert the audio into a Mel-spectrogram and process it as a grey-scale image. The image recognition neural networks are very good at recognising patterns in their first layers. We want our feature extractor to take advantage of these patterns recognition in the first layers. Finally, the last layer of the classifier is trained on the good sounds data. In this work, we transfer learning from three image recognition state-of the-art-models, resnet18~\\cite{he2016deep}, VGG19~\\cite{simonyan2014very} and densenet161~\\cite{huang2017densely}, to sound quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e-YsQrBjzNdX",
    "outputId": "93847adf-092f-496e-8921-8eaa42330cf6"
   },
   "source": [
    "! pip install -U pip\n",
    "! pip install -U torch==1.5.1\n",
    "! pip install -U torchaudio==0.5.1\n",
    "! pip install -U torchvision==0.6.1\n",
    "! pip install -U matplotlib==3.2.1\n",
    "! pip install -U clearml>=0.16.1\n",
    "! pip install -U pandas==1.0.4\n",
    "! pip install -U numpy==1.18.4\n",
    "! pip install -U tensorboard==2.2.1\n",
    "!pip install git+https://github.com/mir-dataset-loaders/mirdata.git@Pedro/good_sounds"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 5.5MB/s \n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-21.0.1\n",
      "Collecting torch==1.5.1\n",
      "  Downloading torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 753.2 MB 14 kB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (1.19.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (0.16.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0+cu101\n",
      "    Uninstalling torch-1.8.0+cu101:\n",
      "      Successfully uninstalled torch-1.8.0+cu101\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.9.0+cu101 requires torch==1.8.0, but you have torch 1.5.1 which is incompatible.\n",
      "torchtext 0.9.0 requires torch==1.8.0, but you have torch 1.5.1 which is incompatible.\u001B[0m\n",
      "Successfully installed torch-1.5.1\n",
      "Collecting torchaudio==0.5.1\n",
      "  Downloading torchaudio-0.5.1-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2 MB 4.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.5.1) (1.5.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->torchaudio==0.5.1) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->torchaudio==0.5.1) (1.19.5)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.5.1\n",
      "Collecting torchvision==0.6.1\n",
      "  Downloading torchvision-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 4.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1) (1.5.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1) (7.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1) (1.19.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->torchvision==0.6.1) (0.16.0)\n",
      "Installing collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.0+cu101\n",
      "    Uninstalling torchvision-0.9.0+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.0+cu101\n",
      "Successfully installed torchvision-0.6.1\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.4 MB 225 kB/s \n",
      "\u001B[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1) (0.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib==3.2.1) (1.15.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001B[0m\n",
      "Successfully installed matplotlib-3.2.1\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.4\n",
      "  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.1 MB 4.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.4) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.4 which is incompatible.\u001B[0m\n",
      "Successfully installed pandas-1.0.4\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pandas"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.18.4\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.2 MB 43.2 MB/s \n",
      "\u001B[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.9.0 requires torch==1.8.0, but you have torch 1.5.1 which is incompatible.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.18.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.4 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001B[0m\n",
      "Successfully installed numpy-1.18.4\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Collecting tensorboard==2.2.1\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 4.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (54.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (0.36.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.18.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (0.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (0.4.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (3.12.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.32.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.2.1) (1.27.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.2.1) (3.7.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.1) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.1) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard==2.2.1) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard==2.2.1) (3.7.4.3)\n",
      "Installing collected packages: tensorboard\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.18.4 which is incompatible.\n",
      "tensorflow 2.4.1 requires tensorboard~=2.4, but you have tensorboard 2.2.1 which is incompatible.\u001B[0m\n",
      "Successfully installed tensorboard-2.2.1\n",
      "Collecting git+https://github.com/mir-dataset-loaders/mirdata.git@Pedro/good_sounds\n",
      "  Cloning https://github.com/mir-dataset-loaders/mirdata.git (to revision Pedro/good_sounds) to /tmp/pip-req-build-d8bzem56\n",
      "  Running command git clone -q https://github.com/mir-dataset-loaders/mirdata.git /tmp/pip-req-build-d8bzem56\n",
      "  Running command git checkout -b Pedro/good_sounds --track origin/Pedro/good_sounds\n",
      "  Switched to a new branch 'Pedro/good_sounds'\n",
      "  Branch 'Pedro/good_sounds' set up to track remote branch 'Pedro/good_sounds' from 'origin'.\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mirdata==0.3.2) (4.41.1)\n",
      "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mirdata==0.3.2) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from mirdata==0.3.2) (1.18.4)\n",
      "Collecting jams\n",
      "  Downloading jams-0.3.4.tar.gz (51 kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 55 kB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from mirdata==0.3.2) (2.23.0)\n",
      "Collecting pretty_midi>=0.2.8\n",
      "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.6 MB 6.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (0.22.2.post1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (0.51.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (1.4.1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (2.1.9)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (1.3.0)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (0.10.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (1.0.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (0.2.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata==0.3.2) (4.4.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->mirdata==0.3.2) (54.1.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->mirdata==0.3.2) (0.34.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->mirdata==0.3.2) (20.9)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->mirdata==0.3.2) (1.4.4)\n",
      "Collecting mido>=1.1.16\n",
      "  Downloading mido-1.2.9-py2.py3-none-any.whl (52 kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52 kB 1.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi>=0.2.8->mirdata==0.3.2) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa>=0.8.0->mirdata==0.3.2) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa>=0.8.0->mirdata==0.3.2) (2.20)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jams->mirdata==0.3.2) (1.0.4)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from jams->mirdata==0.3.2) (2.3.0)\n",
      "Collecting jsonschema>=3.0.0\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 3.2 MB/s \n",
      "\u001B[?25hCollecting mir_eval>=0.5\n",
      "  Downloading mir_eval-0.6.tar.gz (87 kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87 kB 5.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata==0.3.2) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata==0.3.2) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata==0.3.2) (3.7.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval>=0.5->jams->mirdata==0.3.2) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema>=3.0.0->jams->mirdata==0.3.2) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema>=3.0.0->jams->mirdata==0.3.2) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa>=0.8.0->mirdata==0.3.2) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas->jams->mirdata==0.3.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jams->mirdata==0.3.2) (2018.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata==0.3.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata==0.3.2) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata==0.3.2) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata==0.3.2) (1.24.3)\n",
      "Building wheels for collected packages: mirdata, pretty-midi, jams, mir-eval\n",
      "  Building wheel for mirdata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mirdata: filename=mirdata-0.3.2-py3-none-any.whl size=7351944 sha256=2a6e1b393a3a173573843dbd7fb14c93d6bfa050867ab3a325da106c81d57fdf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u0fhtoas/wheels/aa/62/de/3301592297af107e8287c9cb6f796a8facbb642717b869ea36\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591952 sha256=bf1dfbf96fdbcbc650c8203f68f65ddd1e66bc81fff8d955c7d43c7148bb62fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
      "  Building wheel for jams (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for jams: filename=jams-0.3.4-py3-none-any.whl size=64925 sha256=6ef47c4443d9a3363bee49c445c851d724a05d997e8c78bc9f7eb06ee8496773\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/aa/16/ce72bc4caa58dfab819e3f46b3542f2bf90a83009f4ea07a48\n",
      "  Building wheel for mir-eval (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mir-eval: filename=mir_eval-0.6-py3-none-any.whl size=96514 sha256=8a6089d046c6be3cea2959df6166e857b4555e2dbba88607a72681bd8b7f08b1\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/28/2d/006dbad29550bac8daf049ff34fa882655a7d3e77f3b67595e\n",
      "Successfully built mirdata pretty-midi jams mir-eval\n",
      "Installing collected packages: mir-eval, mido, jsonschema, pretty-midi, jams, mirdata\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 2.6.0\n",
      "    Uninstalling jsonschema-2.6.0:\n",
      "      Successfully uninstalled jsonschema-2.6.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbclient 0.5.3 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001B[0m\n",
      "Successfully installed jams-0.3.4 jsonschema-3.2.0 mido-1.2.9 mir-eval-0.6 mirdata-0.3.2 pretty-midi-0.2.9\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E6B2jFg1spuf"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aoz186CXqxFh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7337578-652c-459c-f948-f6fd7672202b"
   },
   "source": [
    "!nvcc -V"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T7T0Rf26zNdm"
   },
   "source": [
    "import PIL\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchaudio\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import models\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9FhfyEMOmWUy"
   },
   "source": [
    "\n",
    "configuration_dict = {'number_of_epochs': 20, 'batch_size': 8, 'dropout': 0.3, 'base_lr': 0.005, \n",
    "                      'number_of_mel_filters': 64, 'resample_freq': 22050}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "msiz7QdvzNeA",
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2e182093-e27a-419d-f2db-025ffce7d7d8"
   },
   "source": [
    "import mirdata\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "g = mirdata.initialize(\"good_sounds\", data_home=\"drive/MyDrive/good_sounds\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW9JfTJsO2s7"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wXtmZe7yzNeS"
   },
   "source": [
    "import sklearn \n",
    "import random\n",
    "\n",
    "def get_data(dataset, klasses):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    sound_selected = []\n",
    "    track_ids, tracks = [], []\n",
    "    if klasses == \"all\":\n",
    "        for k, t in dataset.load_tracks().items():\n",
    "            if t.get_sound_info['klass'] and t.get_sound_info['id'] not in sound_selected:\n",
    "                track_ids.append(k)\n",
    "                tracks.append(t)\n",
    "                sound_selected.append(t.get_sound_info['id'])\n",
    "    else:\n",
    "        for klass in klasses:\n",
    "          for k, t in dataset.load_tracks().items():\n",
    "              if t.get_sound_info['instrument'] == klass and t.get_sound_info['klass'] and t.get_sound_info['id'] not in sound_selected:\n",
    "                  track_ids.append(k)\n",
    "                  tracks.append(t)\n",
    "                  sound_selected.append(t.get_sound_info['id'])\n",
    "    return track_ids, tracks\n",
    "\n",
    "\n",
    "class good_soundsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mirdataset,\n",
    "        seq_duration=0.5,\n",
    "        random_start=True,\n",
    "        resample=8000,\n",
    "        subset=0,\n",
    "        train_split=0.8,\n",
    "        test_split=0.2,\n",
    "        random_seed=42,\n",
    "        klasses=[\"violin\"]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.klasses = klasses # a 'klass' or 'all' or 'family'\n",
    "        self.seq_duration = seq_duration\n",
    "        self.dataset = mirdataset\n",
    "        track_ids, tracks = get_data(self.dataset, self.klasses)\n",
    "        self.track_ids = track_ids\n",
    "        self.tracks = tracks\n",
    "        self.resample = resample\n",
    "        self.set = subset\n",
    "        self.random_start = random_start\n",
    "\n",
    "        #### build a list with labels\n",
    "        self.labels = {label:i for i,label in enumerate(['good', 'bad'])}\n",
    "        full_labels = [x.get_sound_info['klass'] for x in tracks]\n",
    "        #### build the three subsets: train, validation, test using train_test_split, a stratified split with the labels\n",
    "        self.trackids_train, self.trackids_test = sklearn.model_selection.train_test_split(self.track_ids, train_size=1-test_split, random_state=random_seed, stratify=full_labels)\n",
    "        train_labels = [l for l,i in zip(full_labels, self.track_ids) if i in self.trackids_train]\n",
    "        self.trackids_train, self.trackids_valid = sklearn.model_selection.train_test_split(self.trackids_train, train_size=train_split, random_state=random_seed, stratify=train_labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #### get the file with index in the corresponding subset\n",
    "        if self.set==0:\n",
    "            track_id = self.trackids_train[index]\n",
    "        elif self.set==1:\n",
    "            track_id = self.trackids_valid[index]\n",
    "        elif self.set==2:\n",
    "            track_id = self.trackids_test[index]\n",
    "        track = self.dataset.track(track_id)\n",
    "\n",
    "        #### compute start and end frames to read from the disk\n",
    "        si, ei = torchaudio.info(track.audio_path)\n",
    "        sample_rate, channels, length = si.rate, si.channels, si.length\n",
    "        duration = length / sample_rate\n",
    "        if self.seq_duration>duration:\n",
    "            offset = 0\n",
    "            num_frames = length\n",
    "        else:\n",
    "            if self.random_start:\n",
    "                #### we skip a number of seconds at the beginning of file \n",
    "                start = random.uniform(0, duration - self.seq_duration)\n",
    "            else:\n",
    "                start = 0.\n",
    "            #### seconds to audio frames\n",
    "            offset = int(np.floor(start * sample_rate))\n",
    "            num_frames = int(np.floor(self.seq_duration * sample_rate))\n",
    "\n",
    "\n",
    "        #### get audio frames corresponding to offset and num_frames from the disk\n",
    "        audio_signal, sample_rate = torchaudio.load(filepath=track.audio_path, offset=offset,num_frames=num_frames)\n",
    "\n",
    "        #### zero pad if the size is smaller than seq_duration\n",
    "        seq_duration_samples = int(self.seq_duration * sample_rate)\n",
    "        total_samples = audio_signal.shape[-1]\n",
    "        if seq_duration_samples>total_samples:\n",
    "            audio_signal = torch.nn.ConstantPad2d((0,seq_duration_samples-total_samples,0,0),0)(audio_signal)\n",
    "\n",
    "        #### resample\n",
    "        audio_signal = torchaudio.transforms.Resample(sample_rate, self.resample)(audio_signal)\n",
    "\n",
    "        # This will convert audio files with two channels into one\n",
    "        audio_signal = torch.mean(audio_signal, dim=0, keepdim=True)\n",
    "               \n",
    "        # Convert audio to log-scale Mel spectrogram\n",
    "        melspectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.resample, n_mels=64)\n",
    "        melspectrogram = melspectrogram_transform(audio_signal)\n",
    "        melspectogram_db = torchaudio.transforms.AmplitudeToDB()(melspectrogram)\n",
    "        \n",
    "        #Make sure all spectrograms are the same size\n",
    "        fixed_length = 3 * (self.resample//200)\n",
    "        if melspectogram_db.shape[2] < fixed_length:\n",
    "            melspectogram_db = torch.nn.functional.pad(melspectogram_db, (0, fixed_length - melspectogram_db.shape[2]))\n",
    "        else:\n",
    "            melspectogram_db = melspectogram_db[:, :, :fixed_length]\n",
    "\n",
    "        # if self.audio[0]:\n",
    "        #     fixed_length = 3 * self.resample\n",
    "        #     if audio_signal.numel() < fixed_length:\n",
    "        #         audio_signal = torch.nn.functional.pad(audio_signal, (0, fixed_length - audio_signal.numel())).numpy()\n",
    "        #     else:\n",
    "        #         audio_signal = audio_signal[0,:fixed_length].reshape(1,fixed_length).numpy()\n",
    "        # else:\n",
    "        #     audio_signal = np.array([])\n",
    "        audio_signal = np.array([])\n",
    "\n",
    "        return audio_signal, self.resample, torch.cat([melspectogram_db, melspectogram_db, melspectogram_db], dim=0), self.labels['good' if track.get_sound_info['klass'].startswith('good') else 'bad']\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.set==0:\n",
    "            return len(self.trackids_train)\n",
    "        elif self.set==1:\n",
    "            return len(self.trackids_valid)\n",
    "        else:\n",
    "            return len(self.trackids_test)\n",
    "\n",
    "random_seed=0\n",
    "\n",
    "train_dataset = good_soundsDataset(mirdataset=g, subset=0, random_seed=random_seed)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=4,pin_memory=True)\n",
    "valid_dataset = good_soundsDataset(mirdataset=g, subset=1, random_seed=random_seed)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=64,num_workers=4,pin_memory=True)\n",
    "test_dataset = good_soundsDataset(mirdataset=g, subset=2, random_seed=random_seed)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,num_workers=4,pin_memory=True)\n",
    "\n",
    "classes = ('good', 'bad')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "RejIji8dylPZ",
    "outputId": "9c5c61b7-82cc-4ac9-9e94-f76530d019ec"
   },
   "source": [
    "\"kk\""
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'kk'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 35
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_5I49BxPEPB"
   },
   "source": [
    "## Transfer learning from resnet18"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q9cbwiaKmWU0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "34493a939e5d4a71b6b10d0f96130cf3",
      "4f5195375efd46509ce2de6ae8f77bd3",
      "3a4970e6ffc64c41b0e6f94a3d86b24e",
      "3f6a7227d6a040d98a38e4cd0c5760ee",
      "d882e319ec744f989554171cc867a348",
      "c63a1295d3ef4d0bbc626a4aad3621f5",
      "3769ee44482b4ead80efeff58eaa8eb9",
      "48b1546a464a4642a257fa1e92bee31f"
     ]
    },
    "outputId": "d799d449-e783-4b66-f1fd-efee5d7591ba"
   },
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(1, model.conv1.out_channels, kernel_size=model.conv1.kernel_size[0], \n",
    "#                       stride=model.conv1.stride[0], padding=model.conv1.padding[0])\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(*[nn.Dropout(p=configuration_dict.get('dropout', 0.25)), nn.Linear(num_ftrs, len(classes))])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34493a939e5d4a71b6b10d0f96130cf3",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3yKYru14zNef"
   },
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = configuration_dict.get('base_lr', 0.001), momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = configuration_dict.get('number_of_epochs')//3, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wgu-Mp2umWU2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4df2a881-54d2-49b2-f453-0badc8ee5590"
   },
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device to use: {}'.format(device))\n",
    "model.to(device)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Device to use: 0\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5AkpNfJjmWU2"
   },
   "source": [
    "tensorboard_writer = SummaryWriter('./tensorboard_logs')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g-ZZ03KKmWU3"
   },
   "source": [
    "def plot_signal(signal, title, cmap=None):\n",
    "    fig = plt.figure()\n",
    "    if signal.ndim == 1:\n",
    "        plt.plot(signal)\n",
    "    else:\n",
    "        plt.imshow(signal, cmap=cmap)    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plot_buf = io.BytesIO()\n",
    "    plt.savefig(plot_buf, format='jpeg')\n",
    "    plot_buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return ToTensor()(PIL.Image.open(plot_buf))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vdthqz3JzNem"
   },
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (sounds, sample_rate, inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration = epoch * len(train_loader) + batch_idx\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(inputs), len(train_loader.dataset), \n",
    "                          100. * batch_idx / len(train_loader), loss))\n",
    "            tensorboard_writer.add_scalar('training loss/loss', loss, iteration)\n",
    "            tensorboard_writer.add_scalar('learning rate/lr', optimizer.param_groups[0]['lr'], iteration)\n",
    "                \n",
    "        \n",
    "        if batch_idx % debug_interval == 0:    # report debug image every \"debug_interval\" mini-batches\n",
    "            for n, (inp, pred, label) in enumerate(zip(inputs, predicted, labels)):\n",
    "                series = 'label_{}_pred_{}'.format(classes[label.cpu()], classes[pred.cpu()])\n",
    "                # tensorboard_writer.add_image('Train MelSpectrogram samples/{}_{}_{}'.format(batch_idx, n, series), \n",
    "                #                              plot_signal(inp.cpu().numpy().squeeze(), series, 'hot'), iteration)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LBWoj7u5zNes"
   },
   "source": [
    "import sklearn\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    class_correct = list(0. for i in range(2))\n",
    "    class_total = list(0. for i in range(2))\n",
    "    true = []\n",
    "    predict = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (sounds, sample_rate, inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true = true + labels.tolist()\n",
    "            predict = predict + predicted.tolist()\n",
    "            c = (predicted == labels)\n",
    "            for i in range(len(inputs)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "        \n",
    "            iteration = (epoch + 1) * len(train_loader)\n",
    "\n",
    "    total_accuracy = 100 * sum(class_correct)/sum(class_total)\n",
    "    balanced_accuracy = sklearn.metrics.balanced_accuracy_score(true, predict)\n",
    "    f_score = sklearn.metrics.f1_score(true, predict)\n",
    "    precision = sklearn.metrics.precision_score(true, predict)\n",
    "    recall = sklearn.metrics.recall_score(true, predict)\n",
    "    cm = sklearn.metrics.confusion_matrix(true, predict)\n",
    "    print('[Iteration {}] Accuracy on the {} test audios: {}%\\n'.format(epoch, sum(class_total), total_accuracy))\n",
    "    print('[Iteration {}] balanced accuracy on the {} test audios: {}%\\n'.format(epoch, sum(class_total), balanced_accuracy))\n",
    "    print('[Iteration {}] f_score on the {} test audios: {}%\\n'.format(epoch, sum(class_total), f_score))\n",
    "    print('[Iteration {}] precision on the {} test audios: {}%\\n'.format(epoch, sum(class_total), precision))\n",
    "    print('[Iteration {}] recall on the {} test audios: {}%\\n'.format(epoch, sum(class_total), recall))\n",
    "    print('[Iteration {}] cm on the {} test audios: {}%\\n'.format(epoch, sum(class_total), cm))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X5lx3g_5zNey",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3c3757c-6e84-4d65-9400-77eb6e25357a"
   },
   "source": [
    "log_interval = 10\n",
    "debug_interval = 25\n",
    "for epoch in range(configuration_dict.get('number_of_epochs', 10)):\n",
    "    print(epoch)\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "0\n",
      "Train Epoch: 0 [0/884 (0%)]\tLoss: 0.493354\n",
      "Train Epoch: 0 [640/884 (71%)]\tLoss: 0.480135\n",
      "[Iteration 0] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 0] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 0] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 0] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 0] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 0] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "1\n",
      "Train Epoch: 1 [0/884 (0%)]\tLoss: 0.497629\n",
      "Train Epoch: 1 [640/884 (71%)]\tLoss: 0.386270\n",
      "[Iteration 1] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 1] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 1] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 1] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 1] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 1] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "2\n",
      "Train Epoch: 2 [0/884 (0%)]\tLoss: 0.428202\n",
      "Train Epoch: 2 [640/884 (71%)]\tLoss: 0.401294\n",
      "[Iteration 2] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 2] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 2] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 2] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 2] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 2] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "3\n",
      "Train Epoch: 3 [0/884 (0%)]\tLoss: 0.311032\n",
      "Train Epoch: 3 [640/884 (71%)]\tLoss: 0.269374\n",
      "[Iteration 3] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 3] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 3] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 3] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 3] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 3] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "4\n",
      "Train Epoch: 4 [0/884 (0%)]\tLoss: 0.396937\n",
      "Train Epoch: 4 [640/884 (71%)]\tLoss: 0.278218\n",
      "[Iteration 4] Accuracy on the 277.0 test audios: 84.83754512635379%\n",
      "\n",
      "[Iteration 4] balanced accuracy on the 277.0 test audios: 0.5116279069767442%\n",
      "\n",
      "[Iteration 4] f_score on the 277.0 test audios: 0.9176470588235294%\n",
      "\n",
      "[Iteration 4] precision on the 277.0 test audios: 0.8478260869565217%\n",
      "\n",
      "[Iteration 4] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 4] cm on the 277.0 test audios: [[  1  42]\n",
      " [  0 234]]%\n",
      "\n",
      "5\n",
      "Train Epoch: 5 [0/884 (0%)]\tLoss: 0.282998\n",
      "Train Epoch: 5 [640/884 (71%)]\tLoss: 0.330388\n",
      "[Iteration 5] Accuracy on the 277.0 test audios: 85.92057761732852%\n",
      "\n",
      "[Iteration 5] balanced accuracy on the 277.0 test audios: 0.574985092426953%\n",
      "\n",
      "[Iteration 5] f_score on the 277.0 test audios: 0.9221556886227544%\n",
      "\n",
      "[Iteration 5] precision on the 277.0 test audios: 0.8651685393258427%\n",
      "\n",
      "[Iteration 5] recall on the 277.0 test audios: 0.9871794871794872%\n",
      "\n",
      "[Iteration 5] cm on the 277.0 test audios: [[  7  36]\n",
      " [  3 231]]%\n",
      "\n",
      "6\n",
      "Train Epoch: 6 [0/884 (0%)]\tLoss: 0.216081\n",
      "Train Epoch: 6 [640/884 (71%)]\tLoss: 0.295365\n",
      "[Iteration 6] Accuracy on the 277.0 test audios: 86.64259927797833%\n",
      "\n",
      "[Iteration 6] balanced accuracy on the 277.0 test audios: 0.6267143709004174%\n",
      "\n",
      "[Iteration 6] f_score on the 277.0 test audios: 0.9249492900608518%\n",
      "\n",
      "[Iteration 6] precision on the 277.0 test audios: 0.8803088803088803%\n",
      "\n",
      "[Iteration 6] recall on the 277.0 test audios: 0.9743589743589743%\n",
      "\n",
      "[Iteration 6] cm on the 277.0 test audios: [[ 12  31]\n",
      " [  6 228]]%\n",
      "\n",
      "7\n",
      "Train Epoch: 7 [0/884 (0%)]\tLoss: 0.277451\n",
      "Train Epoch: 7 [640/884 (71%)]\tLoss: 0.252968\n",
      "[Iteration 7] Accuracy on the 277.0 test audios: 87.00361010830325%\n",
      "\n",
      "[Iteration 7] balanced accuracy on the 277.0 test audios: 0.6383422778771616%\n",
      "\n",
      "[Iteration 7] f_score on the 277.0 test audios: 0.9268292682926831%\n",
      "\n",
      "[Iteration 7] precision on the 277.0 test audios: 0.8837209302325582%\n",
      "\n",
      "[Iteration 7] recall on the 277.0 test audios: 0.9743589743589743%\n",
      "\n",
      "[Iteration 7] cm on the 277.0 test audios: [[ 13  30]\n",
      " [  6 228]]%\n",
      "\n",
      "8\n",
      "Train Epoch: 8 [0/884 (0%)]\tLoss: 0.205880\n",
      "Train Epoch: 8 [640/884 (71%)]\tLoss: 0.235026\n",
      "[Iteration 8] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 8] balanced accuracy on the 277.0 test audios: 0.5664380838799443%\n",
      "\n",
      "[Iteration 8] f_score on the 277.0 test audios: 0.9134808853118712%\n",
      "\n",
      "[Iteration 8] precision on the 277.0 test audios: 0.8631178707224335%\n",
      "\n",
      "[Iteration 8] recall on the 277.0 test audios: 0.9700854700854701%\n",
      "\n",
      "[Iteration 8] cm on the 277.0 test audios: [[  7  36]\n",
      " [  7 227]]%\n",
      "\n",
      "9\n",
      "Train Epoch: 9 [0/884 (0%)]\tLoss: 0.238428\n",
      "Train Epoch: 9 [640/884 (71%)]\tLoss: 0.241162\n",
      "[Iteration 9] Accuracy on the 277.0 test audios: 87.00361010830325%\n",
      "\n",
      "[Iteration 9] balanced accuracy on the 277.0 test audios: 0.6098688133571855%\n",
      "\n",
      "[Iteration 9] f_score on the 277.0 test audios: 0.927710843373494%\n",
      "\n",
      "[Iteration 9] precision on the 277.0 test audios: 0.875%\n",
      "\n",
      "[Iteration 9] recall on the 277.0 test audios: 0.9871794871794872%\n",
      "\n",
      "[Iteration 9] cm on the 277.0 test audios: [[ 10  33]\n",
      " [  3 231]]%\n",
      "\n",
      "10\n",
      "Train Epoch: 10 [0/884 (0%)]\tLoss: 0.214839\n",
      "Train Epoch: 10 [640/884 (71%)]\tLoss: 0.239301\n",
      "[Iteration 10] Accuracy on the 277.0 test audios: 85.92057761732852%\n",
      "\n",
      "[Iteration 10] balanced accuracy on the 277.0 test audios: 0.593967402106937%\n",
      "\n",
      "[Iteration 10] f_score on the 277.0 test audios: 0.9215291750503017%\n",
      "\n",
      "[Iteration 10] precision on the 277.0 test audios: 0.870722433460076%\n",
      "\n",
      "[Iteration 10] recall on the 277.0 test audios: 0.9786324786324786%\n",
      "\n",
      "[Iteration 10] cm on the 277.0 test audios: [[  9  34]\n",
      " [  5 229]]%\n",
      "\n",
      "11\n",
      "Train Epoch: 11 [0/884 (0%)]\tLoss: 0.235179\n",
      "Train Epoch: 11 [640/884 (71%)]\tLoss: 0.219516\n",
      "[Iteration 11] Accuracy on the 277.0 test audios: 87.00361010830325%\n",
      "\n",
      "[Iteration 11] balanced accuracy on the 277.0 test audios: 0.6573245875571457%\n",
      "\n",
      "[Iteration 11] f_score on the 277.0 test audios: 0.9262295081967213%\n",
      "\n",
      "[Iteration 11] precision on the 277.0 test audios: 0.889763779527559%\n",
      "\n",
      "[Iteration 11] recall on the 277.0 test audios: 0.9658119658119658%\n",
      "\n",
      "[Iteration 11] cm on the 277.0 test audios: [[ 15  28]\n",
      " [  8 226]]%\n",
      "\n",
      "12\n",
      "Train Epoch: 12 [0/884 (0%)]\tLoss: 0.179873\n",
      "Train Epoch: 12 [640/884 (71%)]\tLoss: 0.204015\n",
      "[Iteration 12] Accuracy on the 277.0 test audios: 87.72563176895306%\n",
      "\n",
      "[Iteration 12] balanced accuracy on the 277.0 test audios: 0.66159809183065%\n",
      "\n",
      "[Iteration 12] f_score on the 277.0 test audios: 0.9306122448979591%\n",
      "\n",
      "[Iteration 12] precision on the 277.0 test audios: 0.890625%\n",
      "\n",
      "[Iteration 12] recall on the 277.0 test audios: 0.9743589743589743%\n",
      "\n",
      "[Iteration 12] cm on the 277.0 test audios: [[ 15  28]\n",
      " [  6 228]]%\n",
      "\n",
      "13\n",
      "Train Epoch: 13 [0/884 (0%)]\tLoss: 0.199023\n",
      "Train Epoch: 13 [640/884 (71%)]\tLoss: 0.206978\n",
      "[Iteration 13] Accuracy on the 277.0 test audios: 88.4476534296029%\n",
      "\n",
      "[Iteration 13] balanced accuracy on the 277.0 test audios: 0.7133273703041145%\n",
      "\n",
      "[Iteration 13] f_score on the 277.0 test audios: 0.9336099585062241%\n",
      "\n",
      "[Iteration 13] precision on the 277.0 test audios: 0.907258064516129%\n",
      "\n",
      "[Iteration 13] recall on the 277.0 test audios: 0.9615384615384616%\n",
      "\n",
      "[Iteration 13] cm on the 277.0 test audios: [[ 20  23]\n",
      " [  9 225]]%\n",
      "\n",
      "14\n",
      "Train Epoch: 14 [0/884 (0%)]\tLoss: 0.173848\n",
      "Train Epoch: 14 [640/884 (71%)]\tLoss: 0.197803\n",
      "[Iteration 14] Accuracy on the 277.0 test audios: 88.4476534296029%\n",
      "\n",
      "[Iteration 14] balanced accuracy on the 277.0 test audios: 0.6658715961041543%\n",
      "\n",
      "[Iteration 14] f_score on the 277.0 test audios: 0.934959349593496%\n",
      "\n",
      "[Iteration 14] precision on the 277.0 test audios: 0.8914728682170543%\n",
      "\n",
      "[Iteration 14] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 14] cm on the 277.0 test audios: [[ 15  28]\n",
      " [  4 230]]%\n",
      "\n",
      "15\n",
      "Train Epoch: 15 [0/884 (0%)]\tLoss: 0.203707\n",
      "Train Epoch: 15 [640/884 (71%)]\tLoss: 0.193124\n",
      "[Iteration 15] Accuracy on the 277.0 test audios: 88.4476534296029%\n",
      "\n",
      "[Iteration 15] balanced accuracy on the 277.0 test audios: 0.6658715961041543%\n",
      "\n",
      "[Iteration 15] f_score on the 277.0 test audios: 0.934959349593496%\n",
      "\n",
      "[Iteration 15] precision on the 277.0 test audios: 0.8914728682170543%\n",
      "\n",
      "[Iteration 15] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 15] cm on the 277.0 test audios: [[ 15  28]\n",
      " [  4 230]]%\n",
      "\n",
      "16\n",
      "Train Epoch: 16 [0/884 (0%)]\tLoss: 0.191716\n",
      "Train Epoch: 16 [640/884 (71%)]\tLoss: 0.214999\n",
      "[Iteration 16] Accuracy on the 277.0 test audios: 87.72563176895306%\n",
      "\n",
      "[Iteration 16] balanced accuracy on the 277.0 test audios: 0.6331246273106739%\n",
      "\n",
      "[Iteration 16] f_score on the 277.0 test audios: 0.9314516129032258%\n",
      "\n",
      "[Iteration 16] precision on the 277.0 test audios: 0.8816793893129771%\n",
      "\n",
      "[Iteration 16] recall on the 277.0 test audios: 0.9871794871794872%\n",
      "\n",
      "[Iteration 16] cm on the 277.0 test audios: [[ 12  31]\n",
      " [  3 231]]%\n",
      "\n",
      "17\n",
      "Train Epoch: 17 [0/884 (0%)]\tLoss: 0.168435\n",
      "Train Epoch: 17 [640/884 (71%)]\tLoss: 0.176232\n",
      "[Iteration 17] Accuracy on the 277.0 test audios: 88.8086642599278%\n",
      "\n",
      "[Iteration 17] balanced accuracy on the 277.0 test audios: 0.6774995030808983%\n",
      "\n",
      "[Iteration 17] f_score on the 277.0 test audios: 0.9368635437881874%\n",
      "\n",
      "[Iteration 17] precision on the 277.0 test audios: 0.8949416342412452%\n",
      "\n",
      "[Iteration 17] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 17] cm on the 277.0 test audios: [[ 16  27]\n",
      " [  4 230]]%\n",
      "\n",
      "18\n",
      "Train Epoch: 18 [0/884 (0%)]\tLoss: 0.203401\n",
      "Train Epoch: 18 [640/884 (71%)]\tLoss: 0.180652\n",
      "[Iteration 18] Accuracy on the 277.0 test audios: 85.1985559566787%\n",
      "\n",
      "[Iteration 18] balanced accuracy on the 277.0 test audios: 0.6086762075134169%\n",
      "\n",
      "[Iteration 18] f_score on the 277.0 test audios: 0.9164969450101832%\n",
      "\n",
      "[Iteration 18] precision on the 277.0 test audios: 0.8754863813229572%\n",
      "\n",
      "[Iteration 18] recall on the 277.0 test audios: 0.9615384615384616%\n",
      "\n",
      "[Iteration 18] cm on the 277.0 test audios: [[ 11  32]\n",
      " [  9 225]]%\n",
      "\n",
      "19\n",
      "Train Epoch: 19 [0/884 (0%)]\tLoss: 0.237262\n",
      "Train Epoch: 19 [640/884 (71%)]\tLoss: 0.276540\n",
      "[Iteration 19] Accuracy on the 277.0 test audios: 87.00361010830325%\n",
      "\n",
      "[Iteration 19] balanced accuracy on the 277.0 test audios: 0.6288511230371695%\n",
      "\n",
      "[Iteration 19] f_score on the 277.0 test audios: 0.9271255060728745%\n",
      "\n",
      "[Iteration 19] precision on the 277.0 test audios: 0.8807692307692307%\n",
      "\n",
      "[Iteration 19] recall on the 277.0 test audios: 0.9786324786324786%\n",
      "\n",
      "[Iteration 19] cm on the 277.0 test audios: [[ 12  31]\n",
      " [  5 229]]%\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-nX4BbDPT5C"
   },
   "source": [
    "## Transfer learning from resnet18"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNOBlTNYUIUm",
    "outputId": "ef9b6fef-d83d-462a-fafe-7f2bbbb9b5e5"
   },
   "source": [
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "print(model)\n",
    "num_ftrs = model.fc.in_features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Sequential(*[nn.Dropout(p=configuration_dict.get('dropout', 0.25)), nn.Linear(num_ftrs, len(classes))])\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = configuration_dict.get('base_lr', 0.001), momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = configuration_dict.get('number_of_epochs')//3, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device to use: {}'.format(device))\n",
    "model.to(device)\n",
    "print(\"\")\n",
    "\n",
    "log_interval = 10\n",
    "debug_interval = 25\n",
    "for epoch in range(configuration_dict.get('number_of_epochs', 10)):\n",
    "    print(epoch)\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "Device to use: 0\n",
      "\n",
      "0\n",
      "Train Epoch: 0 [0/884 (0%)]\tLoss: 0.530702\n",
      "Train Epoch: 0 [640/884 (71%)]\tLoss: 0.471104\n",
      "[Iteration 0] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 0] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 0] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 0] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 0] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 0] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "1\n",
      "Train Epoch: 1 [0/884 (0%)]\tLoss: 0.446347\n",
      "Train Epoch: 1 [640/884 (71%)]\tLoss: 0.530173\n",
      "[Iteration 1] Accuracy on the 277.0 test audios: 83.03249097472924%\n",
      "\n",
      "[Iteration 1] balanced accuracy on the 277.0 test audios: 0.5199264559729676%\n",
      "\n",
      "[Iteration 1] f_score on the 277.0 test audios: 0.906187624750499%\n",
      "\n",
      "[Iteration 1] precision on the 277.0 test audios: 0.850187265917603%\n",
      "\n",
      "[Iteration 1] recall on the 277.0 test audios: 0.9700854700854701%\n",
      "\n",
      "[Iteration 1] cm on the 277.0 test audios: [[  3  40]\n",
      " [  7 227]]%\n",
      "\n",
      "2\n",
      "Train Epoch: 2 [0/884 (0%)]\tLoss: 0.423917\n",
      "Train Epoch: 2 [640/884 (71%)]\tLoss: 0.380807\n",
      "[Iteration 2] Accuracy on the 277.0 test audios: 83.39350180505416%\n",
      "\n",
      "[Iteration 2] balanced accuracy on the 277.0 test audios: 0.5125720532697277%\n",
      "\n",
      "[Iteration 2] f_score on the 277.0 test audios: 0.9087301587301586%\n",
      "\n",
      "[Iteration 2] precision on the 277.0 test audios: 0.8481481481481481%\n",
      "\n",
      "[Iteration 2] recall on the 277.0 test audios: 0.9786324786324786%\n",
      "\n",
      "[Iteration 2] cm on the 277.0 test audios: [[  2  41]\n",
      " [  5 229]]%\n",
      "\n",
      "3\n",
      "Train Epoch: 3 [0/884 (0%)]\tLoss: 0.460521\n",
      "Train Epoch: 3 [640/884 (71%)]\tLoss: 0.289042\n",
      "[Iteration 3] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 3] balanced accuracy on the 277.0 test audios: 0.5%\n",
      "\n",
      "[Iteration 3] f_score on the 277.0 test audios: 0.9158512720156556%\n",
      "\n",
      "[Iteration 3] precision on the 277.0 test audios: 0.8447653429602888%\n",
      "\n",
      "[Iteration 3] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 3] cm on the 277.0 test audios: [[  0  43]\n",
      " [  0 234]]%\n",
      "\n",
      "4\n",
      "Train Epoch: 4 [0/884 (0%)]\tLoss: 0.473670\n",
      "Train Epoch: 4 [640/884 (71%)]\tLoss: 0.332276\n",
      "[Iteration 4] Accuracy on the 277.0 test audios: 84.83754512635379%\n",
      "\n",
      "[Iteration 4] balanced accuracy on the 277.0 test audios: 0.5116279069767442%\n",
      "\n",
      "[Iteration 4] f_score on the 277.0 test audios: 0.9176470588235294%\n",
      "\n",
      "[Iteration 4] precision on the 277.0 test audios: 0.8478260869565217%\n",
      "\n",
      "[Iteration 4] recall on the 277.0 test audios: 1.0%\n",
      "\n",
      "[Iteration 4] cm on the 277.0 test audios: [[  1  42]\n",
      " [  0 234]]%\n",
      "\n",
      "5\n",
      "Train Epoch: 5 [0/884 (0%)]\tLoss: 0.431062\n",
      "Train Epoch: 5 [640/884 (71%)]\tLoss: 0.457500\n",
      "[Iteration 5] Accuracy on the 277.0 test audios: 81.2274368231047%\n",
      "\n",
      "[Iteration 5] balanced accuracy on the 277.0 test audios: 0.5187338501291989%\n",
      "\n",
      "[Iteration 5] f_score on the 277.0 test audios: 0.8947368421052632%\n",
      "\n",
      "[Iteration 5] precision on the 277.0 test audios: 0.85%\n",
      "\n",
      "[Iteration 5] recall on the 277.0 test audios: 0.9444444444444444%\n",
      "\n",
      "[Iteration 5] cm on the 277.0 test audios: [[  4  39]\n",
      " [ 13 221]]%\n",
      "\n",
      "6\n",
      "Train Epoch: 6 [0/884 (0%)]\tLoss: 0.389014\n",
      "Train Epoch: 6 [640/884 (71%)]\tLoss: 0.360491\n",
      "[Iteration 6] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "[Iteration 6] balanced accuracy on the 277.0 test audios: 0.526336712383224%\n",
      "\n",
      "[Iteration 6] f_score on the 277.0 test audios: 0.9126984126984128%\n",
      "\n",
      "[Iteration 6] precision on the 277.0 test audios: 0.8518518518518519%\n",
      "\n",
      "[Iteration 6] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 6] cm on the 277.0 test audios: [[  3  40]\n",
      " [  4 230]]%\n",
      "\n",
      "7\n",
      "Train Epoch: 7 [0/884 (0%)]\tLoss: 0.436271\n",
      "Train Epoch: 7 [640/884 (71%)]\tLoss: 0.445052\n",
      "[Iteration 7] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "[Iteration 7] balanced accuracy on the 277.0 test audios: 0.5358278672232161%\n",
      "\n",
      "[Iteration 7] f_score on the 277.0 test audios: 0.9123505976095617%\n",
      "\n",
      "[Iteration 7] precision on the 277.0 test audios: 0.8544776119402985%\n",
      "\n",
      "[Iteration 7] recall on the 277.0 test audios: 0.9786324786324786%\n",
      "\n",
      "[Iteration 7] cm on the 277.0 test audios: [[  4  39]\n",
      " [  5 229]]%\n",
      "\n",
      "8\n",
      "Train Epoch: 8 [0/884 (0%)]\tLoss: 0.386530\n",
      "Train Epoch: 8 [640/884 (71%)]\tLoss: 0.414046\n",
      "[Iteration 8] Accuracy on the 277.0 test audios: 83.03249097472924%\n",
      "\n",
      "[Iteration 8] balanced accuracy on the 277.0 test audios: 0.5199264559729676%\n",
      "\n",
      "[Iteration 8] f_score on the 277.0 test audios: 0.906187624750499%\n",
      "\n",
      "[Iteration 8] precision on the 277.0 test audios: 0.850187265917603%\n",
      "\n",
      "[Iteration 8] recall on the 277.0 test audios: 0.9700854700854701%\n",
      "\n",
      "[Iteration 8] cm on the 277.0 test audios: [[  3  40]\n",
      " [  7 227]]%\n",
      "\n",
      "9\n",
      "Train Epoch: 9 [0/884 (0%)]\tLoss: 0.388665\n",
      "Train Epoch: 9 [640/884 (71%)]\tLoss: 0.429821\n",
      "[Iteration 9] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 9] balanced accuracy on the 277.0 test audios: 0.5379646193599682%\n",
      "\n",
      "[Iteration 9] f_score on the 277.0 test audios: 0.9145129224652087%\n",
      "\n",
      "[Iteration 9] precision on the 277.0 test audios: 0.8550185873605948%\n",
      "\n",
      "[Iteration 9] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 9] cm on the 277.0 test audios: [[  4  39]\n",
      " [  4 230]]%\n",
      "\n",
      "10\n",
      "Train Epoch: 10 [0/884 (0%)]\tLoss: 0.396600\n",
      "Train Epoch: 10 [640/884 (71%)]\tLoss: 0.356305\n",
      "[Iteration 10] Accuracy on the 277.0 test audios: 84.83754512635379%\n",
      "\n",
      "[Iteration 10] balanced accuracy on the 277.0 test audios: 0.5306102166567283%\n",
      "\n",
      "[Iteration 10] f_score on the 277.0 test audios: 0.91699604743083%\n",
      "\n",
      "[Iteration 10] precision on the 277.0 test audios: 0.8529411764705882%\n",
      "\n",
      "[Iteration 10] recall on the 277.0 test audios: 0.9914529914529915%\n",
      "\n",
      "[Iteration 10] cm on the 277.0 test audios: [[  3  40]\n",
      " [  2 232]]%\n",
      "\n",
      "11\n",
      "Train Epoch: 11 [0/884 (0%)]\tLoss: 0.460246\n",
      "Train Epoch: 11 [640/884 (71%)]\tLoss: 0.385395\n",
      "[Iteration 11] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "[Iteration 11] balanced accuracy on the 277.0 test audios: 0.516845557543232%\n",
      "\n",
      "[Iteration 11] f_score on the 277.0 test audios: 0.9130434782608696%\n",
      "\n",
      "[Iteration 11] precision on the 277.0 test audios: 0.8492647058823529%\n",
      "\n",
      "[Iteration 11] recall on the 277.0 test audios: 0.9871794871794872%\n",
      "\n",
      "[Iteration 11] cm on the 277.0 test audios: [[  2  41]\n",
      " [  3 231]]%\n",
      "\n",
      "12\n",
      "Train Epoch: 12 [0/884 (0%)]\tLoss: 0.480480\n",
      "Train Epoch: 12 [640/884 (71%)]\tLoss: 0.308889\n",
      "[Iteration 12] Accuracy on the 277.0 test audios: 86.28158844765343%\n",
      "\n",
      "[Iteration 12] balanced accuracy on the 277.0 test audios: 0.567630689723713%\n",
      "\n",
      "[Iteration 12] f_score on the 277.0 test audios: 0.9246031746031746%\n",
      "\n",
      "[Iteration 12] precision on the 277.0 test audios: 0.8629629629629629%\n",
      "\n",
      "[Iteration 12] recall on the 277.0 test audios: 0.9957264957264957%\n",
      "\n",
      "[Iteration 12] cm on the 277.0 test audios: [[  6  37]\n",
      " [  1 233]]%\n",
      "\n",
      "13\n",
      "Train Epoch: 13 [0/884 (0%)]\tLoss: 0.494303\n",
      "Train Epoch: 13 [640/884 (71%)]\tLoss: 0.449903\n",
      "[Iteration 13] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 13] balanced accuracy on the 277.0 test audios: 0.5189823096799842%\n",
      "\n",
      "[Iteration 13] f_score on the 277.0 test audios: 0.9151873767258383%\n",
      "\n",
      "[Iteration 13] precision on the 277.0 test audios: 0.8498168498168498%\n",
      "\n",
      "[Iteration 13] recall on the 277.0 test audios: 0.9914529914529915%\n",
      "\n",
      "[Iteration 13] cm on the 277.0 test audios: [[  2  41]\n",
      " [  2 232]]%\n",
      "\n",
      "14\n",
      "Train Epoch: 14 [0/884 (0%)]\tLoss: 0.347227\n",
      "Train Epoch: 14 [640/884 (71%)]\tLoss: 0.366146\n",
      "[Iteration 14] Accuracy on the 277.0 test audios: 85.1985559566787%\n",
      "\n",
      "[Iteration 14] balanced accuracy on the 277.0 test audios: 0.5422381236334725%\n",
      "\n",
      "[Iteration 14] f_score on the 277.0 test audios: 0.9188118811881187%\n",
      "\n",
      "[Iteration 14] precision on the 277.0 test audios: 0.8560885608856088%\n",
      "\n",
      "[Iteration 14] recall on the 277.0 test audios: 0.9914529914529915%\n",
      "\n",
      "[Iteration 14] cm on the 277.0 test audios: [[  4  39]\n",
      " [  2 232]]%\n",
      "\n",
      "15\n",
      "Train Epoch: 15 [0/884 (0%)]\tLoss: 0.430557\n",
      "Train Epoch: 15 [640/884 (71%)]\tLoss: 0.376231\n",
      "[Iteration 15] Accuracy on the 277.0 test audios: 81.94945848375451%\n",
      "\n",
      "[Iteration 15] balanced accuracy on the 277.0 test audios: 0.5040250447227191%\n",
      "\n",
      "[Iteration 15] f_score on the 277.0 test audios: 0.9%\n",
      "\n",
      "[Iteration 15] precision on the 277.0 test audios: 0.8458646616541353%\n",
      "\n",
      "[Iteration 15] recall on the 277.0 test audios: 0.9615384615384616%\n",
      "\n",
      "[Iteration 15] cm on the 277.0 test audios: [[  2  41]\n",
      " [  9 225]]%\n",
      "\n",
      "16\n",
      "Train Epoch: 16 [0/884 (0%)]\tLoss: 0.408339\n",
      "Train Epoch: 16 [640/884 (71%)]\tLoss: 0.407205\n",
      "[Iteration 16] Accuracy on the 277.0 test audios: 82.67148014440433%\n",
      "\n",
      "[Iteration 16] balanced accuracy on the 277.0 test audios: 0.5082985489962234%\n",
      "\n",
      "[Iteration 16] f_score on the 277.0 test audios: 0.9043824701195218%\n",
      "\n",
      "[Iteration 16] precision on the 277.0 test audios: 0.8470149253731343%\n",
      "\n",
      "[Iteration 16] recall on the 277.0 test audios: 0.9700854700854701%\n",
      "\n",
      "[Iteration 16] cm on the 277.0 test audios: [[  2  41]\n",
      " [  7 227]]%\n",
      "\n",
      "17\n",
      "Train Epoch: 17 [0/884 (0%)]\tLoss: 0.375130\n",
      "Train Epoch: 17 [640/884 (71%)]\tLoss: 0.405586\n",
      "[Iteration 17] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 17] balanced accuracy on the 277.0 test audios: 0.5284734645199761%\n",
      "\n",
      "[Iteration 17] f_score on the 277.0 test audios: 0.9148514851485149%\n",
      "\n",
      "[Iteration 17] precision on the 277.0 test audios: 0.8523985239852399%\n",
      "\n",
      "[Iteration 17] recall on the 277.0 test audios: 0.9871794871794872%\n",
      "\n",
      "[Iteration 17] cm on the 277.0 test audios: [[  3  40]\n",
      " [  3 231]]%\n",
      "\n",
      "18\n",
      "Train Epoch: 18 [0/884 (0%)]\tLoss: 0.453982\n",
      "Train Epoch: 18 [640/884 (71%)]\tLoss: 0.383745\n",
      "[Iteration 18] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "[Iteration 18] balanced accuracy on the 277.0 test audios: 0.5569469290399522%\n",
      "\n",
      "[Iteration 18] f_score on the 277.0 test audios: 0.9138276553106214%\n",
      "\n",
      "[Iteration 18] precision on the 277.0 test audios: 0.8603773584905661%\n",
      "\n",
      "[Iteration 18] recall on the 277.0 test audios: 0.9743589743589743%\n",
      "\n",
      "[Iteration 18] cm on the 277.0 test audios: [[  6  37]\n",
      " [  6 228]]%\n",
      "\n",
      "19\n",
      "Train Epoch: 19 [0/884 (0%)]\tLoss: 0.347343\n",
      "Train Epoch: 19 [640/884 (71%)]\tLoss: 0.385552\n",
      "[Iteration 19] Accuracy on the 277.0 test audios: 83.39350180505416%\n",
      "\n",
      "[Iteration 19] balanced accuracy on the 277.0 test audios: 0.5030808984297356%\n",
      "\n",
      "[Iteration 19] f_score on the 277.0 test audios: 0.9090909090909091%\n",
      "\n",
      "[Iteration 19] precision on the 277.0 test audios: 0.8455882352941176%\n",
      "\n",
      "[Iteration 19] recall on the 277.0 test audios: 0.9829059829059829%\n",
      "\n",
      "[Iteration 19] cm on the 277.0 test audios: [[  1  42]\n",
      " [  4 230]]%\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poiFhaHyPZDm"
   },
   "source": [
    "## Transfer learning from densenet161"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hdycd7XEXpAi",
    "outputId": "64443cf5-eb7e-4269-d7b3-2ecf4f90127d"
   },
   "source": [
    "\n",
    "model = models.densenet161(pretrained=True)\n",
    "print(model)\n",
    "num_ftrs = model.classifier.in_features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier = nn.Sequential(*[nn.Dropout(p=configuration_dict.get('dropout', 0.25)), nn.Linear(num_ftrs, len(classes))])\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = configuration_dict.get('base_lr', 0.001), momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = configuration_dict.get('number_of_epochs')//3, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device to use: {}'.format(device))\n",
    "model.to(device)\n",
    "print(\"\")\n",
    "\n",
    "log_interval = 10\n",
    "debug_interval = 25\n",
    "for epoch in range(configuration_dict.get('number_of_epochs', 10)):\n",
    "    print(epoch)\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer33): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer34): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer35): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer36): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2208, out_features=1000, bias=True)\n",
      ")\n",
      "Device to use: 0\n",
      "\n",
      "0\n",
      "Train Epoch: 0 [0/884 (0%)]\tLoss: 0.746978\n",
      "Train Epoch: 0 [640/884 (71%)]\tLoss: 0.566314\n",
      "[Iteration 0] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "1\n",
      "Train Epoch: 1 [0/884 (0%)]\tLoss: 0.537517\n",
      "Train Epoch: 1 [640/884 (71%)]\tLoss: 0.506761\n",
      "[Iteration 1] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "2\n",
      "Train Epoch: 2 [0/884 (0%)]\tLoss: 0.558125\n",
      "Train Epoch: 2 [640/884 (71%)]\tLoss: 0.392064\n",
      "[Iteration 2] Accuracy on the 277.0 test audios: 82.31046931407943%\n",
      "\n",
      "3\n",
      "Train Epoch: 3 [0/884 (0%)]\tLoss: 0.443656\n",
      "Train Epoch: 3 [640/884 (71%)]\tLoss: 0.466090\n",
      "[Iteration 3] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "4\n",
      "Train Epoch: 4 [0/884 (0%)]\tLoss: 0.428568\n",
      "Train Epoch: 4 [640/884 (71%)]\tLoss: 0.363346\n",
      "[Iteration 4] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "5\n",
      "Train Epoch: 5 [0/884 (0%)]\tLoss: 0.668071\n",
      "Train Epoch: 5 [640/884 (71%)]\tLoss: 0.680356\n",
      "[Iteration 5] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "6\n",
      "Train Epoch: 6 [0/884 (0%)]\tLoss: 0.540545\n",
      "Train Epoch: 6 [640/884 (71%)]\tLoss: 0.469869\n",
      "[Iteration 6] Accuracy on the 277.0 test audios: 80.14440433212997%\n",
      "\n",
      "7\n",
      "Train Epoch: 7 [0/884 (0%)]\tLoss: 0.390175\n",
      "Train Epoch: 7 [640/884 (71%)]\tLoss: 0.418522\n",
      "[Iteration 7] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "8\n",
      "Train Epoch: 8 [0/884 (0%)]\tLoss: 0.475913\n",
      "Train Epoch: 8 [640/884 (71%)]\tLoss: 0.531654\n",
      "[Iteration 8] Accuracy on the 277.0 test audios: 82.31046931407943%\n",
      "\n",
      "9\n",
      "Train Epoch: 9 [0/884 (0%)]\tLoss: 0.569305\n",
      "Train Epoch: 9 [640/884 (71%)]\tLoss: 0.406708\n",
      "[Iteration 9] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "10\n",
      "Train Epoch: 10 [0/884 (0%)]\tLoss: 0.488602\n",
      "Train Epoch: 10 [640/884 (71%)]\tLoss: 0.458463\n",
      "[Iteration 10] Accuracy on the 277.0 test audios: 82.31046931407943%\n",
      "\n",
      "11\n",
      "Train Epoch: 11 [0/884 (0%)]\tLoss: 0.481984\n",
      "Train Epoch: 11 [640/884 (71%)]\tLoss: 0.396591\n",
      "[Iteration 11] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "12\n",
      "Train Epoch: 12 [0/884 (0%)]\tLoss: 0.520421\n",
      "Train Epoch: 12 [640/884 (71%)]\tLoss: 0.419070\n",
      "[Iteration 12] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "13\n",
      "Train Epoch: 13 [0/884 (0%)]\tLoss: 0.357575\n",
      "Train Epoch: 13 [640/884 (71%)]\tLoss: 0.340782\n",
      "[Iteration 13] Accuracy on the 277.0 test audios: 85.1985559566787%\n",
      "\n",
      "14\n",
      "Train Epoch: 14 [0/884 (0%)]\tLoss: 0.615816\n",
      "Train Epoch: 14 [640/884 (71%)]\tLoss: 0.405126\n",
      "[Iteration 14] Accuracy on the 277.0 test audios: 85.92057761732852%\n",
      "\n",
      "15\n",
      "Train Epoch: 15 [0/884 (0%)]\tLoss: 0.397679\n",
      "Train Epoch: 15 [640/884 (71%)]\tLoss: 0.415794\n",
      "[Iteration 15] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "16\n",
      "Train Epoch: 16 [0/884 (0%)]\tLoss: 0.510763\n",
      "Train Epoch: 16 [640/884 (71%)]\tLoss: 0.363175\n",
      "[Iteration 16] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "17\n",
      "Train Epoch: 17 [0/884 (0%)]\tLoss: 0.497024\n",
      "Train Epoch: 17 [640/884 (71%)]\tLoss: 0.390632\n",
      "[Iteration 17] Accuracy on the 277.0 test audios: 82.31046931407943%\n",
      "\n",
      "18\n",
      "Train Epoch: 18 [0/884 (0%)]\tLoss: 0.384760\n",
      "Train Epoch: 18 [640/884 (71%)]\tLoss: 0.412168\n",
      "[Iteration 18] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "19\n",
      "Train Epoch: 19 [0/884 (0%)]\tLoss: 0.369007\n",
      "Train Epoch: 19 [640/884 (71%)]\tLoss: 0.436323\n",
      "[Iteration 19] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8P4Ep41Ph0p"
   },
   "source": [
    "## Transfer learning from vgg19"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kb0dPRQbbzT5",
    "outputId": "6ab41ad4-2162-47d5-9bac-d1243f8bc880"
   },
   "source": [
    "\n",
    "model = models.vgg19(pretrained=True)\n",
    "print(model)\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[6] = nn.Sequential(*[nn.Dropout(p=configuration_dict.get('dropout', 0.25)), nn.Linear(num_ftrs, len(classes))])\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = configuration_dict.get('base_lr', 0.001), momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = configuration_dict.get('number_of_epochs')//3, gamma = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device to use: {}'.format(device))\n",
    "model.to(device)\n",
    "print(\"\")\n",
    "\n",
    "log_interval = 10\n",
    "debug_interval = 25\n",
    "for epoch in range(configuration_dict.get('number_of_epochs', 10)):\n",
    "    print(epoch)\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "    scheduler.step()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Device to use: 0\n",
      "\n",
      "0\n",
      "Train Epoch: 0 [0/884 (0%)]\tLoss: 2.520588\n",
      "Train Epoch: 0 [640/884 (71%)]\tLoss: 85.157173\n",
      "[Iteration 0] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "1\n",
      "Train Epoch: 1 [0/884 (0%)]\tLoss: 194.137344\n",
      "Train Epoch: 1 [640/884 (71%)]\tLoss: 60.357491\n",
      "[Iteration 1] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "2\n",
      "Train Epoch: 2 [0/884 (0%)]\tLoss: 104.732544\n",
      "Train Epoch: 2 [640/884 (71%)]\tLoss: 168.743713\n",
      "[Iteration 2] Accuracy on the 277.0 test audios: 54.87364620938628%\n",
      "\n",
      "3\n",
      "Train Epoch: 3 [0/884 (0%)]\tLoss: 223.577606\n",
      "Train Epoch: 3 [640/884 (71%)]\tLoss: 179.305649\n",
      "[Iteration 3] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "4\n",
      "Train Epoch: 4 [0/884 (0%)]\tLoss: 139.604568\n",
      "Train Epoch: 4 [640/884 (71%)]\tLoss: 176.870758\n",
      "[Iteration 4] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "5\n",
      "Train Epoch: 5 [0/884 (0%)]\tLoss: 169.222839\n",
      "Train Epoch: 5 [640/884 (71%)]\tLoss: 144.894516\n",
      "[Iteration 5] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "6\n",
      "Train Epoch: 6 [0/884 (0%)]\tLoss: 161.217346\n",
      "Train Epoch: 6 [640/884 (71%)]\tLoss: 119.838135\n",
      "[Iteration 6] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "7\n",
      "Train Epoch: 7 [0/884 (0%)]\tLoss: 123.337708\n",
      "Train Epoch: 7 [640/884 (71%)]\tLoss: 107.206131\n",
      "[Iteration 7] Accuracy on the 277.0 test audios: 83.03249097472924%\n",
      "\n",
      "8\n",
      "Train Epoch: 8 [0/884 (0%)]\tLoss: 66.655823\n",
      "Train Epoch: 8 [640/884 (71%)]\tLoss: 118.539688\n",
      "[Iteration 8] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "9\n",
      "Train Epoch: 9 [0/884 (0%)]\tLoss: 146.495422\n",
      "Train Epoch: 9 [640/884 (71%)]\tLoss: 85.870018\n",
      "[Iteration 9] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "10\n",
      "Train Epoch: 10 [0/884 (0%)]\tLoss: 104.809784\n",
      "Train Epoch: 10 [640/884 (71%)]\tLoss: 79.683212\n",
      "[Iteration 10] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "11\n",
      "Train Epoch: 11 [0/884 (0%)]\tLoss: 113.689888\n",
      "Train Epoch: 11 [640/884 (71%)]\tLoss: 80.276016\n",
      "[Iteration 11] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n",
      "12\n",
      "Train Epoch: 12 [0/884 (0%)]\tLoss: 79.244682\n",
      "Train Epoch: 12 [640/884 (71%)]\tLoss: 66.975037\n",
      "[Iteration 12] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "13\n",
      "Train Epoch: 13 [0/884 (0%)]\tLoss: 112.860764\n",
      "Train Epoch: 13 [640/884 (71%)]\tLoss: 81.675667\n",
      "[Iteration 13] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "14\n",
      "Train Epoch: 14 [0/884 (0%)]\tLoss: 75.228821\n",
      "Train Epoch: 14 [640/884 (71%)]\tLoss: 70.431450\n",
      "[Iteration 14] Accuracy on the 277.0 test audios: 83.39350180505416%\n",
      "\n",
      "15\n",
      "Train Epoch: 15 [0/884 (0%)]\tLoss: 85.862167\n",
      "Train Epoch: 15 [640/884 (71%)]\tLoss: 59.292336\n",
      "[Iteration 15] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "16\n",
      "Train Epoch: 16 [0/884 (0%)]\tLoss: 110.067131\n",
      "Train Epoch: 16 [640/884 (71%)]\tLoss: 62.809067\n",
      "[Iteration 16] Accuracy on the 277.0 test audios: 84.11552346570397%\n",
      "\n",
      "17\n",
      "Train Epoch: 17 [0/884 (0%)]\tLoss: 123.922302\n",
      "Train Epoch: 17 [640/884 (71%)]\tLoss: 87.046776\n",
      "[Iteration 17] Accuracy on the 277.0 test audios: 83.75451263537906%\n",
      "\n",
      "18\n",
      "Train Epoch: 18 [0/884 (0%)]\tLoss: 69.886795\n",
      "Train Epoch: 18 [640/884 (71%)]\tLoss: 69.483871\n",
      "[Iteration 18] Accuracy on the 277.0 test audios: 82.31046931407943%\n",
      "\n",
      "19\n",
      "Train Epoch: 19 [0/884 (0%)]\tLoss: 124.692383\n",
      "Train Epoch: 19 [640/884 (71%)]\tLoss: 89.986755\n",
      "[Iteration 19] Accuracy on the 277.0 test audios: 84.47653429602889%\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}